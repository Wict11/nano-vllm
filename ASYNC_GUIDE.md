# Nano-vLLM å¼‚æ­¥æµæ°´çº¿å®ç°æŒ‡å—

## ç›®å½•
- [éœ€æ±‚åˆ†æ](#éœ€æ±‚åˆ†æ)
- [æ ¸å¿ƒæ€è·¯](#æ ¸å¿ƒæ€è·¯)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ¨¡å—å®ç°](#æ¨¡å—å®ç°)
- [å…³é”®ç»†èŠ‚](#å…³é”®ç»†èŠ‚)
- [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
- [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)

---

## éœ€æ±‚åˆ†æ

### èƒŒæ™¯é—®é¢˜

åœ¨åŸå§‹çš„ä¸²è¡Œæ‰§è¡Œæ¨¡å¼ä¸­ï¼ŒLLM æ¨ç†æµç¨‹æ˜¯åŒæ­¥é˜»å¡çš„ï¼š

```
Step N:   [schedule N (0.5ms)] â†’ [run N (50ms, GPUé˜»å¡)] â†’ [postprocess N (0.5ms)]
Step N+1: [schedule N+1 (0.5ms)] â†’ [run N+1 (50ms, GPUé˜»å¡)] â†’ [postprocess N+1 (0.5ms)]
```

**å­˜åœ¨çš„é—®é¢˜ï¼š**
1. **CPU ç­‰å¾… GPU**ï¼šschedule å’Œ postprocess å¿…é¡»ç­‰å¾… GPU æ¨ç†å®Œæˆæ‰èƒ½æ‰§è¡Œ
2. **GPU ç­‰å¾… CPU**ï¼šGPU æ¨ç†å®Œæˆåï¼Œéœ€è¦ç­‰å¾… CPU å®Œæˆ schedule ä¸‹ä¸€æ‰¹æ¬¡
3. **ä¸²è¡Œæ‰§è¡Œæµªè´¹**ï¼šCPU è°ƒåº¦å’Œ GPU æ¨ç†æ— æ³•å¹¶è¡Œï¼Œé™ä½äº†æ•´ä½“ååé‡

### æ ¸å¿ƒéœ€æ±‚

**ç›®æ ‡ï¼šå®ç° vLLM v1 é£æ ¼çš„å¼‚æ­¥è°ƒåº¦æµæ°´çº¿**

1. **CPU-GPU å¹¶è¡Œ**ï¼šCPU è°ƒåº¦å’Œ GPU æ¨ç†åº”è¯¥æµæ°´çº¿å¹¶è¡Œæ‰§è¡Œ
2. **çŠ¶æ€ä¸€è‡´æ€§**ï¼šåœ¨ chunked prefill åœºæ™¯ä¸‹ï¼Œå¿…é¡»æ­£ç¡®ç®¡ç† pending çŠ¶æ€
3. **å‘åå…¼å®¹**ï¼šä¿æŒä¸ç°æœ‰ API çš„å…¼å®¹æ€§ï¼Œç”¨æˆ·å¯é€‰æ‹©å¯ç”¨å¼‚æ­¥æ¨¡å¼
4. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå¼‚æ­¥é€»è¾‘ç‹¬ç«‹å°è£…ï¼Œä¸å½±å“ä¸²è¡Œæ¨¡å¼

---

## æ ¸å¿ƒæ€è·¯

### ç†æƒ³çš„æµæ°´çº¿æ‰§è¡Œ

```
æ—¶é—´çº¿å¯¹æ¯”ï¼š

ä¸²è¡Œæ¨¡å¼ï¼ˆåŸå§‹ï¼‰ï¼š
CPU: [sched N] ----ç­‰å¾…GPU---- [post N] [sched N+1] ----ç­‰å¾…GPU---- [post N+1]
GPU:            [===== run N =====]                  [===== run N+1 =====]
æ—¶é—´: 51ms per step

å¼‚æ­¥æ¨¡å¼ï¼ˆä¼˜åŒ–åï¼‰ï¼š
CPU: [post N-1][sched N] [post N][sched N+1] [post N+1][sched N+2]
GPU:            [==== run N ====][=== run N+1 ===][== run N+2 ==]
æ—¶é—´: 50ms per stepï¼ˆCPU è°ƒåº¦ä¸ GPU æ¨ç†é‡å ï¼‰
```

### Chunked Prefill çš„æŒ‘æˆ˜

**çŠ¶æ€ä¾èµ–é—®é¢˜ï¼š**

```python
# schedule() éœ€è¦è¯»å– prefilled_len æ¥å†³å®šè°ƒåº¦å“ªä¸ª chunk
prompt_remaining = seq.num_prompt_tokens - seq.prefilled_len

# postprocess() æ›´æ–° prefilled_len
seq.prefilled_len += chunk_size
```

**å¦‚æœç›´æ¥å¼‚æ­¥ä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

```
é”™è¯¯åœºæ™¯ï¼š
Step N:   schedule(N) è¯»å– prefilled_len=0 â†’ è°ƒåº¦ chunk [0:512]
          run_async(N) å¯åŠ¨ï¼ˆGPU å¼‚æ­¥æ‰§è¡Œä¸­...ï¼‰
          
Step N+1: schedule(N+1) è¯»å– prefilled_len=0  â† è¿˜æ˜¯ 0ï¼ï¼ˆpostprocess è¿˜æ²¡æ‰§è¡Œï¼‰
          â†’ âŒ é‡å¤è°ƒåº¦ chunk [0:512]
```

### è§£å†³æ–¹æ¡ˆï¼šPending çŠ¶æ€ç®¡ç†

**æ ¸å¿ƒæ€æƒ³ï¼š**

ç»´æŠ¤ä¸¤ç§çŠ¶æ€ï¼š
- **Actual Stateï¼ˆå®é™…çŠ¶æ€ï¼‰**ï¼šå·²å®Œæˆæ¨ç†å¹¶å¤„ç†çš„çŠ¶æ€ï¼ˆ`prefilled_len`ï¼‰
- **Pending Stateï¼ˆå¾…å®šçŠ¶æ€ï¼‰**ï¼šå·²è°ƒåº¦ä½†æ¨ç†æœªå®Œæˆçš„çŠ¶æ€ï¼ˆ`pending_prefilled_len`ï¼‰

```python
# è®¡ç®—æœ‰æ•ˆçš„ prefilled_lenï¼ˆåŒ…æ‹¬ pendingï¼‰
effective_prefilled = seq.prefilled_len + pending_prefilled_len
                      â†‘                    â†‘
                   å·²å®Œæˆçš„              å·²è°ƒåº¦ä½†æœªå®Œæˆçš„
```

**çŠ¶æ€è½¬æ¢æµç¨‹ï¼š**

```
Step N:
  schedule(N):
    effective = 0 + 0 = 0
    è°ƒåº¦ chunk [0:512]
    è®°å½•åˆ° pending: chunk_info[seq_id] = 512
  
  run_async(N): GPU å¼‚æ­¥æ‰§è¡Œ chunk [0:512]...
  
Step N+1:
  schedule(N+1):
    effective = 0 + 512 = 512  â† è€ƒè™‘ pending çŠ¶æ€
    è°ƒåº¦ chunk [512:1024]
    è®°å½•åˆ° pending: chunk_info[seq_id] = 512 (æ–°çš„)
  
  postprocess(N):  â† N çš„ GPU æ¨ç†å®Œæˆäº†
    seq.prefilled_len = 512  â† åº”ç”¨ pending â†’ actual
    ä» pending ç§»é™¤å·²å®Œæˆçš„æ‰¹æ¬¡
  
  run_async(N+1): GPU å¼‚æ­¥æ‰§è¡Œ chunk [512:1024]...
```

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LLMEngine                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚enable_asyncâ”‚ = False â†’ Scheduler + ModelRunner (ä¸²è¡Œ)     â”‚
â”‚  â”‚ å‚æ•°é€‰æ‹©    â”‚ = True  â†’ AsyncScheduler + AsyncModelRunner  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¸²è¡Œæ¨¡å¼ï¼ˆåŸå§‹ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler   â”‚ -->  â”‚ ModelRunner  â”‚ -->  â”‚ postprocess  â”‚
â”‚  (åŒæ­¥è°ƒåº¦)   â”‚      â”‚  (é˜»å¡æ‰§è¡Œ)   â”‚      â”‚  (åŒæ­¥æ›´æ–°)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¼‚æ­¥æ¨¡å¼ï¼ˆæ–°å¢ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AsyncScheduler   â”‚ -->  â”‚ AsyncModelRunner   â”‚ -->  â”‚ postprocess  â”‚
â”‚ (pendingç®¡ç†)     â”‚      â”‚ (CUDA Streamå¼‚æ­¥)  â”‚      â”‚ (pendingâ†’actual)â”‚
â”‚ effective_len    â”‚      â”‚ éé˜»å¡å¯åŠ¨          â”‚      â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¨¡å—èŒè´£

| æ¨¡å— | èŒè´£ | å…³é”®ç‰¹æ€§ |
|------|------|----------|
| **AsyncScheduler** | è°ƒåº¦ç®¡ç† + Pending çŠ¶æ€è¿½è¸ª | â€¢ ä½¿ç”¨ `effective_prefilled_len` è°ƒåº¦<br>â€¢ ç»´æŠ¤ `pending_batches` é˜Ÿåˆ—<br>â€¢ postprocess æ—¶åº”ç”¨ pending â†’ actual |
| **AsyncModelRunner** | å¼‚æ­¥æ¨ç†æ‰§è¡Œ | â€¢ ä½¿ç”¨ CUDA Stream éé˜»å¡å¯åŠ¨<br>â€¢ ç»´æŠ¤ `pending_results` é˜Ÿåˆ—<br>â€¢ æä¾› `wait_for_result()` åŒæ­¥ç‚¹ |
| **LLMEngine** | æµæ°´çº¿åè°ƒ | â€¢ æ ¹æ® `enable_async` é€‰æ‹©ç»„ä»¶<br>â€¢ `_step_async()` å®ç°æµæ°´çº¿é€»è¾‘<br>â€¢ å¤„ç†æœ€åä¸€æ‰¹æ¬¡çš„ç‰¹æ®Šé€»è¾‘ |
| **Sequence** | åºåˆ—çŠ¶æ€ | â€¢ æ·»åŠ  `aborted` å±æ€§æ”¯æŒå–æ¶ˆ<br>â€¢ `prefilled_len` ä¸ºå®é™…çŠ¶æ€ |

---

## æ¨¡å—å®ç°

### 1. Sequence ç±»æ‰©å±•

**æ–‡ä»¶ï¼š** `nanovllm/engine/sequence.py`

**ä¿®æ”¹å†…å®¹ï¼š**

```python
class Sequence:
    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        # ... åŸæœ‰ä»£ç  ...
        self.prefilled_len = 0  # å·²å®Œæˆçš„ prefill é•¿åº¦
        self.aborted = False    # æ ‡è®°åºåˆ—æ˜¯å¦è¢«å–æ¶ˆï¼ˆå¼‚æ­¥è°ƒåº¦ä½¿ç”¨ï¼‰
```

**è®¾è®¡è¯´æ˜ï¼š**
- `aborted`ï¼šæ”¯æŒå¼‚æ­¥æ¨¡å¼ä¸‹çš„è¯·æ±‚å–æ¶ˆï¼Œpostprocess æ—¶è¿‡æ»¤å·²å–æ¶ˆçš„åºåˆ—
- ä¿æŒå‘åå…¼å®¹ï¼šä¸²è¡Œæ¨¡å¼ä¸ä½¿ç”¨æ­¤å­—æ®µ

---

### 2. AsyncScheduler å®ç°

**æ–‡ä»¶ï¼š** `nanovllm/engine/async_scheduler.py`

**æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š**

```python
class AsyncScheduler:
    def __init__(self, config: Config):
        # åŸºç¡€è°ƒåº¦ç»„ä»¶
        self.waiting: deque[Sequence] = deque()
        self.running: deque[Sequence] = deque()
        self.block_manager = BlockManager(...)
        
        # å¼‚æ­¥æµæ°´çº¿æ”¯æŒ
        self.pending_batches: deque[Dict] = deque()  # è¿½è¸ªæœªå®Œæˆçš„æ‰¹æ¬¡
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_scheduled": 0,
            "pending_batches": 0,
            "max_pending_batches": 0
        }
```

**å…³é”®æ–¹æ³• 1ï¼šschedule() - ä½¿ç”¨ effective_prefilled_len**

```python
def schedule(self) -> tuple[list[Sequence], bool, int, int]:
    """
    è°ƒåº¦ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
    
    å…³é”®åŒºåˆ«ï¼šä½¿ç”¨ effective_prefilled_lenï¼ˆåŒ…æ‹¬ pendingï¼‰
    """
    scheduled_seqs = []
    batch_chunk_info = {}  # seq_id -> chunk_size
    
    # Prefill è°ƒåº¦
    for seq in self.running:
        # â­ å…³é”®ï¼šè®¡ç®—æœ‰æ•ˆçš„ prefilled_lenï¼ˆåŒ…æ‹¬ pendingï¼‰
        effective_prefilled = self._get_effective_prefilled_len(seq)
        prompt_remaining = seq.num_prompt_tokens - seq.num_cached_tokens - effective_prefilled
        
        if prompt_remaining > 0:
            chunk_size = min(prompt_remaining, CHUNK_SIZE)
            scheduled_seqs.append(seq)
            batch_chunk_info[seq.seq_id] = chunk_size  # è®°å½• chunk ä¿¡æ¯
            break
    
    # Decode è°ƒåº¦
    # ... (é€»è¾‘ä¸ä¸²è¡Œç±»ä¼¼ï¼Œä½†ä½¿ç”¨ effective_prefilled_len)
    
    # â­ è®°å½• pending æ‰¹æ¬¡ä¿¡æ¯
    self.pending_batches.append({
        'is_prefill': has_prefill,
        'chunk_info': batch_chunk_info,  # ç”¨äº postprocess
        'seq_ids': [seq.seq_id for seq in scheduled_seqs]
    })
    
    return scheduled_seqs, has_prefill, num_prefill_tokens, num_decode_tokens
```

**å…³é”®æ–¹æ³• 2ï¼špostprocess() - åº”ç”¨ pending â†’ actual**

```python
def postprocess(self, seqs: list[Sequence], token_ids: list[int], is_prefill: bool):
    """
    å¤„ç†æ¨ç†ç»“æœ
    
    å…³é”®ï¼šä» pending_batches è·å–æ‰¹æ¬¡ä¿¡æ¯ï¼Œåº”ç”¨ pending â†’ actual çŠ¶æ€è½¬æ¢
    """
    # â­ ä» pending é˜Ÿåˆ—è·å–æ‰¹æ¬¡ä¿¡æ¯ï¼ˆFIFOï¼‰
    if not self.pending_batches:
        raise RuntimeError("postprocess called but no pending batches")
    
    batch_info = self.pending_batches.popleft()
    chunk_info = batch_info['chunk_info']
    
    # è¿‡æ»¤å·²å–æ¶ˆçš„åºåˆ—
    active_seqs = [seq for seq in seqs if not seq.aborted]
    
    if is_prefill:
        if len(active_seqs) > 1:
            # æ··åˆæ‰¹æ¬¡ï¼šç¬¬ä¸€ä¸ªæ˜¯ prefillï¼Œå…¶ä½™æ˜¯ decode
            prefill_seq = active_seqs[0]
            if prefill_seq.seq_id in chunk_info:
                chunk_size = chunk_info[prefill_seq.seq_id]
                # â­ åº”ç”¨ pending â†’ actual
                prefill_seq.prefilled_len += chunk_size
            
            # å¤„ç† decode åºåˆ—
            for seq, token_id in zip(active_seqs[1:], token_ids):
                self.block_manager.may_append(seq)
                seq.append_token(token_id)
                # ... æ£€æŸ¥æ˜¯å¦å®Œæˆ ...
        else:
            # çº¯ prefill
            prefill_seq = active_seqs[0]
            if prefill_seq.seq_id in chunk_info:
                chunk_size = chunk_info[prefill_seq.seq_id]
                prefill_seq.prefilled_len += chunk_size
    else:
        # çº¯ decode
        for seq, token_id in zip(active_seqs, token_ids):
            seq.append_token(token_id)
            # ... æ£€æŸ¥æ˜¯å¦å®Œæˆ ...
```

**å…³é”®æ–¹æ³• 3ï¼š_get_effective_prefilled_len() - è®¡ç®—æœ‰æ•ˆé•¿åº¦**

```python
def _get_effective_prefilled_len(self, seq: Sequence) -> int:
    """
    è®¡ç®—æœ‰æ•ˆçš„ prefilled é•¿åº¦ï¼ˆåŒ…æ‹¬ pending çŠ¶æ€ï¼‰
    
    è¿™æ˜¯å¼‚æ­¥è°ƒåº¦çš„å…³é”®ï¼šè€ƒè™‘å·²è°ƒåº¦ä½†æœªå®Œæˆçš„ chunk
    """
    actual_prefilled = seq.prefilled_len
    
    # ç´¯è®¡æ‰€æœ‰ pending æ‰¹æ¬¡ä¸­è¯¥åºåˆ—çš„ chunk_size
    pending_prefilled = 0
    for batch_info in self.pending_batches:
        chunk_info = batch_info.get('chunk_info', {})
        if seq.seq_id in chunk_info:
            pending_prefilled += chunk_info[seq.seq_id]
    
    return actual_prefilled + pending_prefilled
```

**è®¾è®¡äº®ç‚¹ï¼š**
1. **pending_batches æ˜¯ FIFO é˜Ÿåˆ—**ï¼šä¿è¯ postprocess æŒ‰é¡ºåºåº”ç”¨çŠ¶æ€
2. **chunk_info è®°å½•ç²¾ç¡®ä¿¡æ¯**ï¼šæ”¯æŒæ··åˆæ‰¹æ¬¡å’Œå˜é•¿ chunk
3. **ç»Ÿè®¡ä¿¡æ¯**ï¼šæ–¹ä¾¿è°ƒè¯•å’Œæ€§èƒ½åˆ†æ

---

### 3. AsyncModelRunner å®ç°

**æ–‡ä»¶ï¼š** `nanovllm/engine/async_model_runner.py`

**æ ¸å¿ƒè®¾è®¡ï¼š**

```python
class AsyncModelRunner:
    """
    å¼‚æ­¥æ¨¡å‹æ‰§è¡Œå™¨åŒ…è£…å™¨
    
    åŒ…è£…æ ‡å‡† ModelRunnerï¼Œæ·»åŠ å¼‚æ­¥æ‰§è¡Œæ”¯æŒ
    """
    
    def __init__(self, config: Config, rank: int, event: Event | list[Event]):
        # åˆå§‹åŒ–æ ‡å‡†çš„ ModelRunner
        self.model_runner = ModelRunner(config, rank, event)
        
        # â­ åˆ›å»ºç‹¬ç«‹çš„æ¨ç† streamï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if rank == 0:
            self.inference_stream = torch.cuda.Stream()
            self.pending_results = []  # [(result, event, args), ...]
            self.use_async = True
        else:
            self.use_async = False
        
        self.rank = rank
```

**å…³é”®æ–¹æ³• 1ï¼šrun_async() - éé˜»å¡å¯åŠ¨**

```python
def run_async(self, seqs, is_prefill: bool, num_prefill_tokens: int, num_decode_tokens: int) -> None:
    """
    å¼‚æ­¥å¯åŠ¨æ¨ç†ï¼Œç«‹å³è¿”å›ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
    """
    if not self.use_async or self.rank != 0:
        # éä¸»è¿›ç¨‹ç›´æ¥åŒæ­¥æ‰§è¡Œ
        result = self.model_runner.call("run", seqs, is_prefill, num_prefill_tokens, num_decode_tokens)
        self.pending_results = [(result, None, None)]
        return
    
    # â­ åœ¨ç‹¬ç«‹ stream ä¸­å¼‚æ­¥æ‰§è¡Œ
    with torch.cuda.stream(self.inference_stream):
        # æ‰§è¡Œæ¨ç†
        result = self.model_runner.call("run", seqs, is_prefill, num_prefill_tokens, num_decode_tokens)
        
        # â­ åˆ›å»ºåŒæ­¥äº‹ä»¶
        event = torch.cuda.Event()
        event.record(self.inference_stream)
        
        # è®°å½• pending ç»“æœ
        self.pending_results.append((result, event, (seqs, is_prefill)))
    
    # â­ ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾… GPU å®Œæˆ
```

**å…³é”®æ–¹æ³• 2ï¼šwait_for_result() - åŒæ­¥ç‚¹**

```python
def wait_for_result(self) -> Optional[Any]:
    """
    ç­‰å¾…æœ€æ—©çš„æ¨ç†å®Œæˆå¹¶è¿”å›ç»“æœ
    
    Returns:
        æ¨ç†ç»“æœï¼ˆtoken_idsï¼‰
    """
    if not self.pending_results:
        return None
    
    result, event, args = self.pending_results.pop(0)
    
    # â­ åŒæ­¥ç­‰å¾…å®Œæˆ
    if event is not None:
        event.synchronize()  # é˜»å¡ç›´åˆ° GPU æ¨ç†å®Œæˆ
    
    return result
```

**è®¾è®¡äº®ç‚¹ï¼š**
1. **CUDA Stream éš”ç¦»**ï¼šä½¿ç”¨ç‹¬ç«‹ stream é¿å…é˜»å¡é»˜è®¤ stream
2. **Event åŒæ­¥æœºåˆ¶**ï¼šç²¾ç¡®æ§åˆ¶ CPU-GPU åŒæ­¥ç‚¹
3. **åŒ…è£…å™¨æ¨¡å¼**ï¼šå¤ç”¨ç°æœ‰ ModelRunnerï¼Œæœ€å°åŒ–ä»£ç ä¿®æ”¹
4. **å¤šè¿›ç¨‹å…¼å®¹**ï¼šéä¸»è¿›ç¨‹å›é€€åˆ°åŒæ­¥æ¨¡å¼

---

### 4. LLMEngine æµæ°´çº¿åè°ƒ

**æ–‡ä»¶ï¼š** `nanovllm/engine/llm_engine.py`

**åˆå§‹åŒ–ï¼šæ ¹æ® enable_async é€‰æ‹©ç»„ä»¶**

```python
class LLMEngine:
    def __init__(self, model, enable_async: bool = False, **kwargs):
        """
        åˆå§‹åŒ– LLM å¼•æ“
        
        Args:
            enable_async: æ˜¯å¦å¯ç”¨å¼‚æ­¥æµæ°´çº¿æ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼‰
        """
        config = Config(model, **config_kwargs)
        
        # â­ æ¨¡å¼é€‰æ‹©
        self.enable_async = enable_async
        
        # å¯åŠ¨å­è¿›ç¨‹
        for i in range(1, config.tensor_parallel_size):
            runner_class = AsyncModelRunner if enable_async else ModelRunner
            process = ctx.Process(target=runner_class, args=(config, i, event))
            process.start()
            # ...
        
        # ä¸»è¿›ç¨‹ ModelRunner
        if enable_async:
            self.model_runner = AsyncModelRunner(config, 0, self.events)
        else:
            self.model_runner = ModelRunner(config, 0, self.events)
        
        # é€‰æ‹©è°ƒåº¦å™¨
        if enable_async:
            self.scheduler = AsyncScheduler(config)
            self.pending_batch = None  # è¿½è¸ªå¾…å¤„ç†çš„æ‰¹æ¬¡
            print("[LLMEngine] å¼‚æ­¥æµæ°´çº¿æ¨¡å¼å·²å¯ç”¨")
        else:
            self.scheduler = Scheduler(config)
            print("[LLMEngine] ä¸²è¡Œæ¨¡å¼å·²å¯ç”¨")
```

**ä¸²è¡Œæ¨¡å¼ï¼š_step_sync()**

```python
def _step_sync(self):
    """
    ä¸²è¡Œæ‰§è¡Œæ¨¡å¼ï¼ˆåŸå§‹é€»è¾‘ï¼‰
    
    æµç¨‹ï¼šschedule â†’ runï¼ˆé˜»å¡ï¼‰ â†’ postprocess
    """
    # 1. è°ƒåº¦
    seqs, is_prefill, num_prefill_tokens, num_decode_tokens = self.scheduler.schedule()
    
    # 2. åŒæ­¥æ‰§è¡Œæ¨ç†ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
    token_ids = self.model_runner.call("run", seqs, is_prefill, num_prefill_tokens, num_decode_tokens)
    
    # 3. å¤„ç†ç»“æœ
    self.scheduler.postprocess(seqs, token_ids, is_prefill)
    
    # 4. æ”¶é›†è¾“å‡º
    outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
    
    return outputs, num_tokens
```

**å¼‚æ­¥æ¨¡å¼ï¼š_step_async() - æµæ°´çº¿æ ¸å¿ƒ**

```python
def _step_async(self):
    """
    å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œæ¨¡å¼
    
    æµç¨‹ï¼š
    1. å¤„ç†ä¸Šä¸€æ‰¹æ¬¡çš„ç»“æœï¼ˆç­‰å¾… GPU å®Œæˆï¼‰
    2. è°ƒåº¦ä¸‹ä¸€æ‰¹æ¬¡ï¼ˆå¦‚æœè¿˜æœ‰ä»»åŠ¡ï¼‰
    3. å¼‚æ­¥å¯åŠ¨æ¨ç†ï¼ˆç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…ï¼‰
    """
    outputs = []
    num_tokens = 0
    
    # â­ æ­¥éª¤1: å¤„ç†ä¸Šä¸€æ‰¹æ¬¡çš„ç»“æœ
    if self.pending_batch is not None:
        seqs, is_prefill = self.pending_batch
        
        # ç­‰å¾…æ¨ç†å®Œæˆå¹¶è·å–ç»“æœ
        token_ids = self.model_runner.wait_for_result()
        
        if token_ids is not None:
            # å¤„ç†ç»“æœï¼ˆåº”ç”¨ pending â†’ actualï¼‰
            self.scheduler.postprocess(seqs, token_ids, is_prefill)
            
            # æ”¶é›†è¾“å‡º
            outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
            num_tokens = sum(seq.prefilled_len for seq in seqs) if is_prefill else -len(seqs)
        
        self.pending_batch = None
    
    # â­ æ­¥éª¤2: è°ƒåº¦ä¸‹ä¸€æ‰¹æ¬¡ï¼ˆæ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»»åŠ¡ï¼‰
    if self.scheduler.is_finished():
        return outputs, num_tokens
    
    seqs, is_prefill, num_prefill_tokens, num_decode_tokens = self.scheduler.schedule()
    
    # â­ æ­¥éª¤3: å¼‚æ­¥å¯åŠ¨æ¨ç†ï¼ˆç«‹å³è¿”å›ï¼‰
    self.model_runner.run_async(seqs, is_prefill, num_prefill_tokens, num_decode_tokens)
    
    # è®°å½•å¾…å¤„ç†çš„æ‰¹æ¬¡
    self.pending_batch = (seqs, is_prefill)
    
    return outputs, num_tokens
```

**æ‰§è¡Œæ—¶é—´çº¿å¯¹æ¯”ï¼š**

```
ä¸²è¡Œæ¨¡å¼ step():
  [schedule] â†’ [run (é˜»å¡ 50ms)] â†’ [postprocess]
  æ€»è€—æ—¶: 51ms

å¼‚æ­¥æ¨¡å¼ _step_async():
  Step N:   [wait_for_result (N-1)] [postprocess (N-1)] [schedule (N)] [run_async (N) ç«‹å³è¿”å›]
  Step N+1: [wait_for_result (N)]   [postprocess (N)]   [schedule (N+1)] [run_async (N+1)]
  
  CPU: [post N-1][sched N] [post N][sched N+1] (æ¯æ¬¡åªéœ€ 1ms)
  GPU:            [== run N ==][= run N+1 =]   (é‡å æ‰§è¡Œ)
```

---

## å…³é”®ç»†èŠ‚

### ç»†èŠ‚ 1: æœ€åä¸€æ‰¹æ¬¡çš„å¤„ç†

**é—®é¢˜ï¼š**

åœ¨ `generate()` å¾ªç¯ä¸­ï¼Œ`is_finished()` è¿”å› `True` åå°±é€€å‡ºå¾ªç¯ï¼Œä½†å¼‚æ­¥æ¨¡å¼ä¸‹æœ€åä¸€ä¸ªæ‰¹æ¬¡è¿˜åœ¨ `pending_batch` ä¸­æœªå¤„ç†ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

```python
def generate(self, prompts, sampling_params, use_tqdm=True):
    # æ·»åŠ è¯·æ±‚
    for prompt, sp in zip(prompts, sampling_params):
        self.add_request(prompt, sp)
    
    outputs = {}
    
    # ä¸»å¾ªç¯
    while not self.is_finished():
        output, num_tokens = self.step()
        for seq_id, token_ids in output:
            outputs[seq_id] = token_ids
            if use_tqdm:
                pbar.update(1)
    
    # â­ å¼‚æ­¥æ¨¡å¼ä¸‹éœ€è¦å¤„ç†æœ€åä¸€ä¸ª pending æ‰¹æ¬¡
    if self.enable_async and self.pending_batch is not None:
        seqs, is_prefill = self.pending_batch
        token_ids = self.model_runner.wait_for_result()
        if token_ids is not None:
            self.scheduler.postprocess(seqs, token_ids, is_prefill)
            for seq in seqs:
                if seq.is_finished and seq.seq_id not in outputs:
                    outputs[seq.seq_id] = seq.completion_token_ids
                    if use_tqdm:
                        pbar.update(1)
        self.pending_batch = None
    
    # æ’åºè¾“å‡º
    outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]
    return [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} for token_ids in outputs]
```

**åŸå› ï¼š**
- å¼‚æ­¥æ¨¡å¼çš„æµæ°´çº¿è®¾è®¡ä¸­ï¼Œ`step()` è¿”å›çš„æ˜¯ **ä¸Šä¸€æ‰¹æ¬¡** çš„è¾“å‡º
- æœ€åä¸€æ¬¡ `step()` è°ƒåº¦äº†æœ€åä¸€æ‰¹ä½†ç«‹å³è¿”å›ï¼Œè¿˜æœªç­‰å¾…å®Œæˆ
- å¿…é¡»åœ¨å¾ªç¯å¤–æ˜¾å¼ç­‰å¾…å¹¶å¤„ç†æœ€åä¸€æ‰¹

---

### ç»†èŠ‚ 2: æ˜¾å­˜ä¼°ç®—ä¼˜åŒ–

**é—®é¢˜ï¼š**

åœ¨å¤šæ¬¡å®ä¾‹åŒ– LLM æ—¶ï¼ˆå¦‚ç¤ºä¾‹ä¸­å…ˆè¿è¡Œä¸²è¡Œå†è¿è¡Œå¼‚æ­¥ï¼‰ï¼Œç¬¬äºŒæ¬¡å®ä¾‹åŒ–ä¼šå› ä¸ºæ˜¾å­˜ä¼°ç®—ä¸å‡†ç¡®è€Œå¤±è´¥ã€‚

**åŸå› ï¼š**

```python
# æ—§çš„ä¼°ç®—é€»è¾‘ï¼ˆä¸å‡†ç¡®ï¼‰
free, total = torch.cuda.mem_get_info()
used = total - free
peak = torch.cuda.memory_stats()["allocated_bytes.all.peak"]
current = torch.cuda.memory_stats()["allocated_bytes.all.current"]
config.num_kvcache_blocks = int(total * config.gpu_memory_utilization - used - peak + current) // block_bytes
```

é—®é¢˜ï¼š`peak` æ˜¯å³°å€¼å†…å­˜ï¼Œå¯èƒ½åŒ…å«å·²é‡Šæ”¾çš„å†…å­˜ï¼Œå¯¼è‡´ä¼°ç®—è¿‡å°ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

```python
def allocate_kv_cache(self):
    config = self.config
    hf_config = config.hf_config
    
    # â­ æ¸…ç†ç¼“å­˜ï¼Œé¿å…ä¸Šä¸€ä¸ªå®ä¾‹çš„ç¼“å­˜å½±å“ä¼°ç®—
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    torch.cuda.reset_peak_memory_stats()
    
    # é‡æ–°è·å–æ˜¾å­˜çŠ¶æ€ï¼ˆæ¸…ç†åï¼‰
    free, total = torch.cuda.mem_get_info()
    current = torch.cuda.memory_stats()["allocated_bytes.all.current"]
    
    # â­ ä½¿ç”¨æ›´å‡†ç¡®çš„å…¬å¼
    # å¯åˆ†é…æ˜¾å­˜ = æ€»æ˜¾å­˜ Ã— åˆ©ç”¨ç‡ - å½“å‰å·²åˆ†é…
    available_memory = int(total * config.gpu_memory_utilization) - current
    config.num_kvcache_blocks = available_memory // block_bytes
    
    # â­ å›é€€é€»è¾‘ï¼Œé¿å…æ–­è¨€å¤±è´¥
    if config.num_kvcache_blocks <= 0:
        config.num_kvcache_blocks = 1
        print("[WARN] å¯ç”¨æ˜¾å­˜ä¸è¶³ï¼Œå·²å›é€€ä¸º1ä¸ªKVå—...")
```

**æ”¹è¿›ç‚¹ï¼š**
1. æ¸…ç†å‰å…ˆ `synchronize()` ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
2. é‡ç½®å³°å€¼ç»Ÿè®¡ï¼Œé¿å…å†å²æ•°æ®å½±å“
3. ç®€åŒ–å…¬å¼ï¼Œä¸å†ä½¿ç”¨æ··æ·†çš„ `used + peak - current`
4. æ·»åŠ å›é€€é€»è¾‘ï¼Œé¿å…ç›´æ¥å¤±è´¥

---

### ç»†èŠ‚ 3: èµ„æºæ¸…ç†

**é—®é¢˜ï¼š**

ç¤ºä¾‹è„šæœ¬ä¸­å…ˆè¿è¡Œä¸²è¡Œæ¨¡å¼å†è¿è¡Œå¼‚æ­¥æ¨¡å¼ï¼Œéœ€è¦ç¡®ä¿ç¬¬ä¸€ä¸ªå®ä¾‹çš„èµ„æºè¢«å®Œå…¨é‡Šæ”¾ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

```python
def example_sync():
    llm = LLM(model_path, max_model_len=2048, chunk_prefill_size=512)
    try:
        outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
        print(f"\nè¾“å‡º: {outputs[0]['text']}")
    finally:
        # â­ ç¡®ä¿é‡Šæ”¾è¿›ç¨‹/æ˜¾å­˜èµ„æºï¼Œä¾¿äºåç»­é‡æ–°å®ä¾‹åŒ–
        llm.exit()

def example_async():
    llm = LLM(model_path, max_model_len=2048, chunk_prefill_size=512, enable_async=True)
    try:
        outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
        print(f"\nè¾“å‡º: {outputs[0]['text']}")
    finally:
        # â­ ç¡®ä¿é‡Šæ”¾èµ„æºï¼Œé˜²æ­¢ä¸åç»­å®ä¾‹å†²çª
        llm.exit()
```

**LLMEngine.exit() æ”¹è¿›ï¼š**

```python
def exit(self):
    # â­ é˜²æŠ¤ï¼šå¦‚æœæ„é€ å¤±è´¥ï¼Œmodel_runner å¯èƒ½ä¸å­˜åœ¨
    if hasattr(self, "model_runner"):
        try:
            self.model_runner.call("exit")
        finally:
            del self.model_runner
    
    for p in self.ps:
        p.join()  # ç­‰å¾…å­è¿›ç¨‹ç»“æŸ
```

---

### ç»†èŠ‚ 4: ç©ºè°ƒåº¦ä¿æŠ¤

**é—®é¢˜ï¼š**

å¼‚æ­¥æ¨¡å¼ä¸‹ï¼Œpostprocess å®Œæˆåå¯èƒ½æ‰€æœ‰åºåˆ—éƒ½å·²å®Œæˆï¼Œæ­¤æ—¶è°ƒåº¦ä¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œå¯¼è‡´ `assert scheduled_seqs` å¤±è´¥ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

```python
def _step_async(self):
    # ... postprocess ä¸Šä¸€æ‰¹æ¬¡ ...
    
    # â­ è°ƒåº¦å‰æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»»åŠ¡
    if self.scheduler.is_finished():
        return outputs, num_tokens
    
    # å®‰å…¨è°ƒåº¦ï¼ˆæ­¤æ—¶å¿…æœ‰ä»»åŠ¡ï¼‰
    seqs, is_prefill, num_prefill_tokens, num_decode_tokens = self.scheduler.schedule()
    
    # ... å¼‚æ­¥å¯åŠ¨æ¨ç† ...
```

---

### ç»†èŠ‚ 5: Sequence.aborted å±æ€§

**é—®é¢˜ï¼š**

AsyncScheduler ä¸­ä½¿ç”¨äº† `seq.aborted` æ¥è¿‡æ»¤å·²å–æ¶ˆçš„åºåˆ—ï¼Œä½†åŸå§‹ Sequence ç±»æ²¡æœ‰è¿™ä¸ªå±æ€§ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# nanovllm/engine/sequence.py
class Sequence:
    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        # ... åŸæœ‰å­—æ®µ ...
        self.prefilled_len = 0
        self.aborted = False  # â­ æ·»åŠ å–æ¶ˆæ ‡è®°
```

**ä½¿ç”¨åœºæ™¯ï¼š**

```python
# AsyncScheduler.postprocess()
active_seqs = [seq for seq in seqs if not seq.aborted]

# AsyncScheduler.abort_request()
for seq in list(self.running):
    if hasattr(seq, 'request_id') and seq.request_id == request_id:
        seq.aborted = True
        seq.status = SequenceStatus.FINISHED
        self.block_manager.deallocate(seq)
        self.running.remove(seq)
```

---

## ä½¿ç”¨æ–¹å¼

### åŸºæœ¬ç”¨æ³•

```python
from nanovllm import LLM, SamplingParams

# ä¸²è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
llm = LLM(
    model_path,
    max_model_len=2048,
    chunk_prefill_size=512
)

# å¼‚æ­¥æ¨¡å¼ï¼ˆå¯ç”¨æµæ°´çº¿ï¼‰
llm = LLM(
    model_path,
    max_model_len=2048,
    chunk_prefill_size=512,
    enable_async=True  # â† å”¯ä¸€åŒºåˆ«
)

# API å®Œå…¨ç›¸åŒ
outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
```

### å®Œæ•´ç¤ºä¾‹

å‚è§ `example_async_usage.py`ï¼š

```python
def example_sync():
    """ä¸²è¡Œæ¨¡å¼ç¤ºä¾‹ï¼ˆé»˜è®¤ï¼‰"""
    llm = LLM("/path/to/model", max_model_len=2048, chunk_prefill_size=512)
    try:
        prompts = ["Hello, how are you?"]
        outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
        print(f"\nè¾“å‡º: {outputs[0]['text']}")
    finally:
        llm.exit()

def example_async():
    """å¼‚æ­¥æµæ°´çº¿æ¨¡å¼ç¤ºä¾‹"""
    llm = LLM("/path/to/model", max_model_len=2048, chunk_prefill_size=512, enable_async=True)
    try:
        prompts = ["Hello, how are you?"]
        outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
        print(f"\nè¾“å‡º: {outputs[0]['text']}")
        # æŸ¥çœ‹è°ƒåº¦ç»Ÿè®¡
        if hasattr(llm.scheduler, 'get_stats'):
            print(f"\nè°ƒåº¦ç»Ÿè®¡: {llm.scheduler.get_stats()}")
    finally:
        llm.exit()

if __name__ == "__main__":
    example_sync()
    example_async()
```

### å…³é—­ Chunked Prefill

å¦‚æœæƒ³å…³é—­ chunked prefillï¼ˆä¸€æ¬¡æ€§ prefill æ•´ä¸ª promptï¼‰ï¼š

```python
llm = LLM(
    model_path,
    max_model_len=2048,
    chunk_prefill_size=99999,  # è®¾ç½®ä¸ºå¾ˆå¤§çš„å€¼
    enable_async=True
)
```

**æ³¨æ„ï¼š**
- å¼‚æ­¥é€»è¾‘ä»ç„¶æ­£å¸¸å·¥ä½œï¼ˆpending çŠ¶æ€åªä¼šè®°å½•ä¸€æ¬¡å®Œæ•´ prefillï¼‰
- ä½†æµæ°´çº¿æ”¶ç›Šä¼šé™ä½ï¼ˆprefill åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
- ç¡®ä¿ `max_num_batched_tokens` è¶³å¤Ÿå®¹çº³æœ€é•¿ prompt

---

## æ€§èƒ½åˆ†æ

### å®é™…æµ‹è¯•ç»“æœ

```
============================================================
ä¸²è¡Œæ¨¡å¼ç¤ºä¾‹
============================================================
[LLMEngine] ä¸²è¡Œæ¨¡å¼å·²å¯ç”¨
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1.13it/s, Prefill=7tok/s, Decode=274tok/s]

è¾“å‡º:  ğŸ˜Š Hello! I'm glad to hear that! ğŸ˜Š How about you today...

============================================================
å¼‚æ­¥æµæ°´çº¿æ¨¡å¼ç¤ºä¾‹
============================================================
[LLMEngine] å¼‚æ­¥æµæ°´çº¿æ¨¡å¼å·²å¯ç”¨
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 8.41it/s, Prefill=1482tok/s, Decode=29020tok/s]

è¾“å‡º:  Yes, I am, as you and I. Also, I'd like to point out that the...

è°ƒåº¦ç»Ÿè®¡: {'total_scheduled': 21, 'pending_batches': 1, 'max_pending_batches': 1, 
          'waiting': 0, 'running': 0, 'current_pending': 0}
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | ä¸²è¡Œæ¨¡å¼ | å¼‚æ­¥æ¨¡å¼ | æå‡ |
|------|---------|---------|------|
| **Decode åå** | 274 tok/s | 29020 tok/s | 105xï¼ˆç»Ÿè®¡åå·®ï¼‰ |
| **è°ƒåº¦æ¬¡æ•°** | 21 | 21 | ç›¸åŒ |
| **Max Pending** | N/A | 1 | ç¬¦åˆè®¾è®¡ |
| **API å…¼å®¹æ€§** | 100% | 100% | æ— æ”¹å˜ |

### ç»Ÿè®¡è¯´æ˜

**ä¸ºä»€ä¹ˆ Decode ååæ˜¾ç¤ºè¿™ä¹ˆé«˜ï¼Ÿ**

å¼‚æ­¥æ¨¡å¼çš„ååç»Ÿè®¡æœ‰åå·®ï¼š
```python
# ä¸²è¡Œæ¨¡å¼ï¼šè®¡æ—¶åŒ…å«å®Œæ•´æ¨ç†
token_ids = self.model_runner.call("run", ...)  # é˜»å¡ 50ms
decode_throughput = num_tokens / (perf_counter() - t)  # çœŸå®æ—¶é—´

# å¼‚æ­¥æ¨¡å¼ï¼šè®¡æ—¶åªåŒ…å« CPU æ“ä½œ
self.model_runner.run_async(...)  # ç«‹å³è¿”å› < 1ms
decode_throughput = num_tokens / (perf_counter() - t)  # æçŸ­æ—¶é—´ â†’ æé«˜åå
```

**çœŸå®æ”¶ç›Šåœ¨å“ªé‡Œï¼Ÿ**

1. **ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½**ï¼šæ€»è€—æ—¶ä» `51ms * 21 = 1071ms` é™ä½åˆ°çº¦ `50ms * 21 + è°ƒåº¦æ—¶é—´`
2. **GPU åˆ©ç”¨ç‡æå‡**ï¼šGPU æ— éœ€ç­‰å¾… CPU è°ƒåº¦ï¼Œè¿ç»­æ¨ç†
3. **å¤æ‚åœºæ™¯æ”¶ç›Šæ›´æ˜æ˜¾**ï¼š
   - å¤šè¯·æ±‚å¹¶å‘
   - é•¿ prompt + chunked prefill
   - å¤æ‚è°ƒåº¦ç­–ç•¥

### ç†è®ºåˆ†æ

**å•è¯·æ±‚åœºæ™¯ï¼ˆå½“å‰æµ‹è¯•ï¼‰ï¼š**

```
ä¸²è¡Œ: 21 steps Ã— 51ms = 1071ms
å¼‚æ­¥: 21 steps Ã— 50ms = 1050ms
æ”¶ç›Š: ~2% ï¼ˆè°ƒåº¦å¾ˆå¿«ï¼Œé‡å æ”¶ç›Šå°ï¼‰
```

**å¤šè¯·æ±‚åœºæ™¯ï¼ˆæ›´å¤æ‚è°ƒåº¦ï¼‰ï¼š**

å‡è®¾è°ƒåº¦æ—¶é—´å¢åŠ åˆ° 5msï¼š

```
ä¸²è¡Œ: N steps Ã— (5ms + 50ms + 5ms) = N Ã— 60ms
å¼‚æ­¥: N steps Ã— 50ms ï¼ˆè°ƒåº¦ä¸æ¨ç†é‡å ï¼‰
æ”¶ç›Š: ~17%
```

**é•¿ prompt + chunked prefillï¼š**

å‡è®¾ 8192 token promptï¼Œchunk_size=512ï¼Œéœ€è¦ 16 ä¸ª prefill æ­¥éª¤ï¼š

```
ä¸²è¡Œ: 16 prefill Ã— 51ms + M decode Ã— 51ms
å¼‚æ­¥: 16 prefill Ã— 50ms + M decode Ã— 50msï¼ˆé‡å æ‰§è¡Œï¼‰
æ”¶ç›Š: æ¯ä¸ª step èŠ‚çœ 1msï¼Œæ€»è®¡èŠ‚çœ (16 + M) ms
```

---

## è°ƒè¯•å’Œç›‘æ§

### è°ƒåº¦ç»Ÿè®¡

```python
# è·å–è°ƒåº¦ç»Ÿè®¡ä¿¡æ¯
stats = llm.scheduler.get_stats()
print(stats)

# è¾“å‡ºï¼š
# {
#   'total_scheduled': 21,        # æ€»å…±è°ƒåº¦çš„æ‰¹æ¬¡æ•°
#   'pending_batches': 1,         # å³°å€¼ pending æ‰¹æ¬¡æ•°
#   'max_pending_batches': 1,     # æœ€å¤§ pending æ•°
#   'waiting': 0,                 # å½“å‰ waiting é˜Ÿåˆ—é•¿åº¦
#   'running': 0,                 # å½“å‰ running é˜Ÿåˆ—é•¿åº¦
#   'current_pending': 0          # å½“å‰ pending æ‰¹æ¬¡æ•°
# }
```

### å¸¸è§é—®é¢˜æ’æŸ¥

**1. AssertionError: scheduled_seqs ä¸ºç©º**

åŸå› ï¼šè°ƒåº¦æ—¶æ²¡æœ‰ä»»åŠ¡å¯è°ƒåº¦
è§£å†³ï¼šæ£€æŸ¥ `is_finished()` è°ƒç”¨ï¼Œç¡®ä¿åœ¨æœ‰ä»»åŠ¡æ—¶æ‰è°ƒåº¦

**2. AttributeError: 'Sequence' object has no attribute 'aborted'**

åŸå› ï¼šSequence ç±»æœªæ·»åŠ  `aborted` å±æ€§
è§£å†³ï¼šåœ¨ `Sequence.__init__()` ä¸­æ·»åŠ  `self.aborted = False`

**3. æ˜¾å­˜ä¸è¶³è­¦å‘Š**

åŸå› ï¼šæ˜¾å­˜ä¼°ç®—ä¸å‡†ç¡®æˆ–å®é™…å‰©ä½™æ˜¾å­˜ä¸è¶³
è§£å†³ï¼š
- é™ä½ `gpu_memory_utilization`ï¼ˆå¦‚ 0.8ï¼‰
- å‡å° `max_model_len` æˆ– `chunk_prefill_size`
- ç¡®ä¿ä¸Šä¸€ä¸ªå®ä¾‹å·²è°ƒç”¨ `exit()` é‡Šæ”¾èµ„æº

**4. è¾“å‡ºä¸å®Œæ•´**

åŸå› ï¼šå¼‚æ­¥æ¨¡å¼ä¸‹æœ€åä¸€æ‰¹æ¬¡æœªå¤„ç†
è§£å†³ï¼šç¡®ä¿ `generate()` å¾ªç¯åæœ‰å¤„ç† `pending_batch` çš„é€»è¾‘

---

## æ€»ç»“

### å®ç°è¦ç‚¹

1. **Pending çŠ¶æ€ç®¡ç†**ï¼šé€šè¿‡ `effective_prefilled_len` è§£å†³ chunked prefill çŠ¶æ€ä¾èµ–
2. **CUDA Stream å¼‚æ­¥**ï¼šä½¿ç”¨ç‹¬ç«‹ stream å®ç°éé˜»å¡æ¨ç†
3. **æµæ°´çº¿åè°ƒ**ï¼šLLMEngine æ­£ç¡®ç¼–æ’ postprocess â†’ schedule â†’ run_async
4. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå¼‚æ­¥é€»è¾‘ç‹¬ç«‹å°è£…ï¼Œä¸å½±å“ä¸²è¡Œæ¨¡å¼
5. **å‘åå…¼å®¹**ï¼šé€šè¿‡ `enable_async` å‚æ•°é€‰æ‹©ï¼ŒAPI ä¿æŒä¸å˜

### é€‚ç”¨åœºæ™¯

**æ¨èä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼š**
- âœ… å¤šè¯·æ±‚å¹¶å‘å¤„ç†
- âœ… é•¿ prompt + chunked prefill
- âœ… å¤æ‚è°ƒåº¦ç­–ç•¥ï¼ˆè°ƒåº¦è€—æ—¶è¾ƒé•¿ï¼‰
- âœ… å¯¹ååé‡è¦æ±‚é«˜çš„åœºæ™¯

**å¯ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼š**
- âœ… ç®€å•å•è¯·æ±‚åœºæ™¯
- âœ… çŸ­ prompt æ—  chunked prefill
- âœ… è°ƒåº¦é€»è¾‘ç®€å•ï¼ˆ< 1msï¼‰
- âœ… è°ƒè¯•å’Œå¼€å‘é˜¶æ®µ

### æœªæ¥ä¼˜åŒ–æ–¹å‘

1. **å¤šæ‰¹æ¬¡ Pending**ï¼šæ”¯æŒ pending é˜Ÿåˆ—é•¿åº¦ > 1ï¼Œè¿›ä¸€æ­¥æå‡å¹¶å‘
2. **è‡ªé€‚åº”è°ƒåº¦**ï¼šæ ¹æ® GPU å ç”¨ç‡åŠ¨æ€è°ƒæ•´ pending æ·±åº¦
3. **æ›´ç²¾ç»†çš„ç»Ÿè®¡**ï¼šåŒºåˆ† CPU æ—¶é—´å’Œ GPU æ—¶é—´ï¼Œå‡†ç¡®æµ‹é‡æ”¶ç›Š
4. **Mixed-Precision Pipeline**ï¼šä¸åŒç²¾åº¦çš„ prefill å’Œ decode

---

## å‚è€ƒèµ„æ–™

- [vLLM v1 Blog](https://blog.vllm.ai/2024/09/05/perf-update.html)
- [ASYNC_PIPELINE_DESIGN.md](./ASYNC_PIPELINE_DESIGN.md) - è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ
- [ASYNC_ARCHITECTURE.md](./ASYNC_ARCHITECTURE.md) - æ¶æ„è¯´æ˜
- PyTorch CUDA Stream æ–‡æ¡£
